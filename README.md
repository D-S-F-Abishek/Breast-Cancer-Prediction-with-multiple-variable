Benefits of this project: Provides a deeper understanding of how logistic regression works under the hood instead of just relying on ML libraries.
Reinforces knowledge of cost functions, gradients, and optimization techniques.
Helps in visualizing how the model separates classes using decision boundaries.
Builds confidence in writing raw ML code from scratch.



Objectives of the project

1) Logistic Regression from Scratch: Implementation of a multi-feature logistic regression model without high-level ML libraries.

2) Gradient Descent: Custom implementation of gradient descent for optimizing model parameters.

3) Data Normalization: Normalizing features before training to ensure convergence.

4) Sigmoid & Cost Function: Manual implementation of the sigmoid activation and cross-entropy cost function.

5) Decision Boundary Visualization: 2D plots showing how the model classifies benign vs malignant tumors.

6) Interactive Prediction: A command-line interface where users can input new patient feature values and receive predictions.

7) Accuracy Measurement: Calculates accuracy on the training set to evaluate model performance.

Libraries Used

1) Pandas: For dataset loading and preprocessing.

2) NumPy: For numerical computations (mean, std, dot products, etc.).

3) Matplotlib: For plotting sigmoid curves, decision boundaries, and misclassified points.

4) Math & Copy: Used for mathematical operations and safe parameter updates during training.

Dataset



The dataset used is the Breast Cancer Wisconsin (Diagnostic) dataset, available on Kaggle:
ðŸ”— Breast Cancer Dataset on Kaggle
